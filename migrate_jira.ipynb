{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utliz.jm_utilz import *\n",
    "from utliz.jira_config import *\n",
    "\n",
    "import warnings\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Suppress only the InsecureRequestWarning\n",
    "warnings.simplefilter('ignore', InsecureRequestWarning)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migration Process Steps:\n",
    "1. Disable on-prem ability to submit new records or update current.\n",
    "2. Use the on-prem migration assistant to migrate only users/groups\n",
    "3. While user/groups are being migrated, use the utliz.jm_delete.py to delete all issue records in the cloud instance\n",
    "4. After the jm_delete process has started deleting records, start the CSV build process\n",
    "5. After all cloud records have been deleted and the new CSV has been built, load the new CSV into the cloud\n",
    "6. Validate all CSV records have been migrated to the cloud\n",
    "7. After the CSV has been loaded, update the comments to the on-prem state then load attachements then update the Google reference docs  \n",
    "8. Update all required DNS to point to new instance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a database connetion object that is used to extract the required data from views built from the backend Jira tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sys_info_services = SysInfoServices(jconfig['db_server'],\n",
    "                                     jconfig['database'],\n",
    "                                     jconfig['db_username'],\n",
    "                                     jconfig['db_password'])\n",
    "\n",
    "\n",
    "api_token = jconfig['api_token']\n",
    "url = jconfig['url']\n",
    "jira_session = Jira(url, jconfig['jira_user_name'], api_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Custom CSV to Load into the Cloud\n",
    "### Note: comments made in code to help identify locations for custom updates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mirgration view and format the data \n",
    "##### ------------------> this assumes the SQL tables were built using the files found in the SQL folder so the view names listed below would match those records\n",
    "all_issues = _sys_info_services.get_table_or_view_data('vw_all_jira_issues') # all jira issues\n",
    "all_issues['Issue_ID'] = all_issues['Issue_ID'].astype(int)\n",
    "all_issues['Issue_Ticket_Number'] = all_issues['Issue_Ticket_Number'].astype(int)\n",
    "all_issues['Issue_Priority'] = all_issues['Issue_Priority'].astype(int)\n",
    "all_issues['Issue_Status_Category'] = all_issues['Issue_Status_Category'].astype(int)\n",
    "all_issues['Parent_Issue_ID'] =  pd.to_numeric(all_issues['Parent_Issue_ID'], errors='coerce').astype('Int64')\n",
    "all_issues['TIMEORIGINALESTIMATE'] =  pd.to_numeric(all_issues['TIMEORIGINALESTIMATE'], errors='coerce').astype('Int64')\n",
    "all_issues['TIMEESTIMATE'] =  pd.to_numeric(all_issues['TIMEESTIMATE'], errors='coerce').astype('Int64')\n",
    "all_issues['TIMESPENT'] =  pd.to_numeric(all_issues['TIMESPENT'], errors='coerce').astype('Int64')\n",
    "\n",
    "fields_to_drop = ['Assignee_User_Is_Active',\n",
    "                  'Creator_User_Is_Active',\n",
    "                  'Issue_Status_ID',\n",
    "                  'Issue_Status_Description',\n",
    "                  'Issue_Status_Category',\n",
    "                  'Issue_Status_Icon_Relative_Url',\n",
    "                  'Issue_Type_Is_SubTask',\n",
    "                  'Issue_Type_Icon_Relative_Url',\n",
    "                  'Resolution_ID',\n",
    "                  'Reporter_User_Is_Active',\n",
    "                  'Issue_Priority',\n",
    "                  'Assignee_User_Display_Name',\n",
    "                  'Creator_UserName',\n",
    "                  'Creator_User_Display_Name',\n",
    "                  #'Issue_Description',\n",
    "                  'Issue_Relative_URL',\n",
    "                  'Issue_Type_Description',\n",
    "                  'Issue_Type_ID',\n",
    "                  'Resolution_Description']\n",
    "\n",
    "all_issues = all_issues.drop(columns=fields_to_drop)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cf_data = _sys_info_services.get_table_or_view_data('vw_custom_field_data') # custom field data for issues\n",
    "cf_data['custom_field_value_record_id'] = cf_data['custom_field_value_record_id'].astype(int)\n",
    "cf_data['ISSUE'] = cf_data['ISSUE'].astype(int)\n",
    "cf_data['CUSTOMFIELD'] = cf_data['CUSTOMFIELD'].astype(int)\n",
    "cf_data['ID'] = cf_data['ID'].astype(int)\n",
    "cf_data['PARENTKEY'] =  pd.to_numeric(cf_data['PARENTKEY'], errors='coerce').astype('Int64')\n",
    "\n",
    "cf_options = _sys_info_services.get_table_or_view_data('vw_custom_field_options') # mapping table to determine what the human readable translation of coded values\n",
    "cf_options['ID'] = cf_options['ID'].astype(int)\n",
    "cf_options['CUSTOMFIELD'] = cf_options['CUSTOMFIELD'].astype(int)\n",
    "cf_options['CUSTOMFIELDCONFIG'] = cf_options['CUSTOMFIELDCONFIG'].astype(int)\n",
    "cf_options['SEQUENCE'] = cf_options['SEQUENCE'].astype(int)\n",
    "\n",
    "\n",
    "watchers = _sys_info_services.get_table_or_view_data('vw_jira_watchers')\n",
    "watchers['Issue_ID'] = watchers['issue_id'].astype(int)\n",
    "\n",
    "\n",
    "worklogsdf = _sys_info_services.get_table_or_view_data('vw_all_worklogs')\n",
    "worklogsdf['issueid'] = worklogsdf['issueid'].astype(int)\n",
    "worklogsdf['ID'] = worklogsdf['ID'].astype(int) \n",
    "worklogsdf = update_time_for_worklog(worklogsdf)\n",
    "worklogsdf['STARTDATE'] = worklogsdf['STARTDATE'].astype(str) \n",
    "\n",
    "\n",
    "commentsdf = _sys_info_services.get_table_or_view_data('vw_jira_comments')\n",
    "commentsdf['Issue_ID'] = commentsdf['Issue_ID'].astype(int)\n",
    "commentsdf['Issue_Ticket_Number'] = commentsdf['Issue_Ticket_Number'].astype(int) \n",
    "commentsdf['issuenum'] = commentsdf['issuenum'].astype(int)\n",
    "commentsdf['unique_comment_id'] = commentsdf['unique_comment_id'].astype(int)\n",
    "commentsdf['Parent_Issue_ID'] =  pd.to_numeric(commentsdf['Parent_Issue_ID'], errors='coerce').astype('Int64')\n",
    "\n",
    "\n",
    "# loop through the folder that contains the migration records sets from users and extract the contents\n",
    "issue_folder_path = jconfig['issue_folder_path'] # path where excel files that contain records provided by agents for what jira records they need migrated \n",
    "issue_error_path = jconfig['issue_error_path'] # path where error reports will be stored \n",
    "issue_file_list = list_files_in_folder(issue_folder_path)\n",
    "issue_errors_list = list_files_in_folder(issue_error_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# delete error records if they exist\n",
    "if len(issue_errors_list) > 0:\n",
    "    print('deleting prior error files')\n",
    "    for i in issue_errors_list:\n",
    "        delete_file(i['file_path'])\n",
    "else:\n",
    "    print('no prior error files found to delete')\n",
    "\n",
    "\n",
    "\n",
    "issue_hold_df = []\n",
    "for i in issue_file_list:\n",
    "    # print(i['file_path'], ' <- excel path')\n",
    "    df = pd.read_excel(i['file_path'])\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # qc to ensure parent records exist for a child record\n",
    "    df, invalid_rows = check_if_parent_record_exists(df)  \n",
    "\n",
    "    # if there are child records that do not have the parent record in the set the \n",
    "    # save off the error report \n",
    "    if len(invalid_rows) > 0:\n",
    "        print(i['file_name'], ' missing parent records for the following file. ')\n",
    "        error_path = issue_error_path+'missing_parents_'+i['file_name']\n",
    "        invalid_rows.to_excel(error_path, index=False)\n",
    "\n",
    "    issue_hold_df.append(df)\n",
    "\n",
    "\n",
    "issues_concat = pd.concat(issue_hold_df)\n",
    "issues_concat = issues_concat.drop_duplicates().reset_index(drop=True)\n",
    "print(len(issues_concat), ' <-- Number of issues from team members')\n",
    "\n",
    "\n",
    "\n",
    "def get_project(input_string):\n",
    "    # Find the position of the hyphen\n",
    "    hyphen_pos = input_string.find('-')\n",
    "\n",
    "    # If the hyphen is found, return the substring before it\n",
    "    if hyphen_pos != -1:\n",
    "        return input_string[:hyphen_pos]\n",
    "    else:\n",
    "        # Return the original string if no hyphen is found\n",
    "        return input_string\n",
    "\n",
    "\n",
    "# build a project key \"RDS or UTT or etc\"\n",
    "issues_concat['project'] = issues_concat.apply(lambda x: get_project(x['Issue Key']), axis=1)\n",
    "\n",
    "# only projects to keep for migration \n",
    "keep_projects = ['RDS','UTT'] # <-------------------------------------------------------- This will need to be updated\n",
    "issues_concat = issues_concat[issues_concat.project.isin(keep_projects)]\n",
    "\n",
    "# get a list of the issues that require migration\n",
    "target_issues_list = list(set(list(issues_concat['Issue id'])))\n",
    "\n",
    "# get a set of the target issue records from the user supplied excel files\n",
    "target_record_set = all_issues[all_issues.Issue_ID.isin(target_issues_list)].reset_index(drop=True)\n",
    "\n",
    "# use the project keys to add bulk records to the result set, for projects where ALL records are being migrated \n",
    "project_key_list = ['BRS', 'GTT'] #  <--------------------------------------------------------------------------- This will need to be updated\n",
    "filtered_all_issues = all_issues[all_issues.Project_Key.isin(project_key_list)]\n",
    "target_record_set = pd.concat([target_record_set, filtered_all_issues]).reset_index(drop=True)\n",
    "\n",
    "# now get all records created after 1/1/2024 for specific projects that were not include in the bulk load\n",
    "after_date_projects = ['RDS','UTT']   #  <--------------------------------------------------------------------------- This will need to be updated\n",
    "project_filter_df = all_issues[all_issues.Project_Key.isin(after_date_projects)]\n",
    "project_filter_df['date_filter'] = pd.to_datetime(project_filter_df.Created_Date)\n",
    "print(len(project_filter_df), ' <- number of db records before filtering for 1/1/2024 date')\n",
    "target_date = pd.to_datetime('1/1/2024')\n",
    "after_date_records = project_filter_df[project_filter_df.date_filter >= target_date]\n",
    "after_date_records = after_date_records.drop(columns=['date_filter'])\n",
    "print(len(after_date_records), ' <- number of db records after filtering for 1/1/2024 date')\n",
    "\n",
    "print(len(target_record_set), ' <- number of excel and BRS records PRIOR to joining in records filtered for after 1/1/2024 date')\n",
    "target_record_set = pd.concat([target_record_set, after_date_records]).reset_index(drop=True)\n",
    "print(len(target_record_set), ' <- number of excel and BRS records AFTER to joining in records filtered for after 1/1/2024 date')\n",
    "\n",
    "# update the following variable for downstream use\n",
    "target_issues_list = list(target_record_set.Issue_ID) \n",
    "\n",
    "print('')\n",
    "print('**************************')\n",
    "print('')\n",
    "print(len(target_record_set), ' <-- number of matching records from DB')\n",
    "print(f'{target_record_set.Project_Key.unique()}', ' <-- project types included in the set')\n",
    "print(len(target_issues_list), ' <-- sanity check to ensure the target_issues_list matches the count of matching records from the db')\n",
    "print('')\n",
    "print('**************************')\n",
    "print('')\n",
    "\n",
    "\n",
    "print('Start adding custom fields')\n",
    "# get a list of the custom fields from the dict that helps map the custom names where their supporting data is found\n",
    "custom_field_key_list = list(custom_field_dict)\n",
    "\n",
    "# add the custom fields to the issues df\n",
    "for f in custom_field_key_list:\n",
    "    target_record_set[f] = target_record_set.apply(lambda x: build_custom_fields(x.Issue_ID, f, x.Issue_key, cf_data, cf_options, fields_needing_options_data, custom_field_dict), axis=1)\n",
    "\n",
    "print('End adding custom fields')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Start update of Customer Request Type')\n",
    "# apperently the downloaded csv contains a field called \"Customer Request Type\" that need to be updated to match the Request Type IDs found only \n",
    "# via pulling the detail from the cloud AFTER building the fields. The following replaces the values in the \"Customer Request Type\" field with \n",
    "# the id values from the cloud\n",
    "request_types = jira_session.get_request_types(['RDS','BRS','UTT']) #  <---------------------------------------- This will need to be updated with UTT when ready\n",
    "request_dict = df_create_dict(request_types, 'name', 'id')\n",
    "\n",
    "def replace_request_type(text, request_dict):\n",
    "    try:\n",
    "        return int(request_dict[text])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "target_record_set['Customer Request Type'] = target_record_set.apply(lambda x: replace_request_type(x['Customer Request Type'], request_dict), axis=1)\n",
    "target_record_set['Customer Request Type'] =  pd.to_numeric(target_record_set['Customer Request Type'], errors='coerce').astype('Int64')\n",
    "print('End updating Customer Request Type')\n",
    "\n",
    "\n",
    "def contains_non_ascii(value):\n",
    "    \"\"\" Check if the string contains non-ASCII characters \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return any(ord(char) >= 128 for char in value)\n",
    "    return False\n",
    "\n",
    "def find_non_ascii_fields(df):\n",
    "    non_ascii_fields = []\n",
    "    for column in df.columns:\n",
    "        if df[column].apply(contains_non_ascii).any():\n",
    "            non_ascii_fields.append(column)\n",
    "    return non_ascii_fields\n",
    "\n",
    "\n",
    "print('Start formatting records')\n",
    "# preprocess the record set prior to adding the duplicated fields\n",
    "# update the datetime fields \n",
    "update_time_fields = ['Created_Date', 'Updated_Date', 'Resolution_Date', 'Due_Date']\n",
    "for d in update_time_fields:\n",
    "    target_record_set[d] = target_record_set.apply(lambda x: pd.to_datetime(x[d], errors='coerce'), axis=1)\n",
    "    \n",
    "for d in update_time_fields:\n",
    "    target_record_set[d] = target_record_set.apply(lambda x: format_date(x[d]), axis=1)\n",
    "\n",
    "for d in update_time_fields:\n",
    "    try:\n",
    "        target_record_set[d] = pd.to_datetime(target_record_set[d], errors='coerce').dt.strftime(\"%d/%b/%y %I:%M %p\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting datetime for column {d}: {e}\")\n",
    "        target_record_set[d] = ''  # Set an empty string for columns with formatting errors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of columns you want to apply the remove_non_asci function to\n",
    "# columns_to_process = ['Issue_Description', 'Project Description','Issue_Status_Description','Issue_Type_Description',\n",
    "# 'Issue_Summary','Project Title']# 'Change reason','Custom field (Change risk)','Custom field (Impact)', <-- No records were being returned \n",
    "columns_to_process = find_non_ascii_fields(target_record_set)\n",
    "\n",
    "\n",
    "# Apply the remove_non_ascii function to the specified columns\n",
    "target_record_set[columns_to_process] = target_record_set[columns_to_process].applymap(remove_non_ascii)\n",
    "\n",
    "\n",
    "\n",
    "target_record_set = target_record_set.sort_values(by=['Parent_Issue_ID'], ascending=False, na_position='last')\n",
    "target_record_set['Parent_Issue_ID'] = target_record_set['Parent_Issue_ID'].apply(lambda x: '' if pd.isna(x) else x)\n",
    "target_record_set['Due_Date'] = target_record_set.apply(lambda x: '' if isinstance(x['Due_Date'], float) else x['Due_Date'], axis=1)\n",
    "target_record_set['Resolution_Date'] = target_record_set.apply(lambda x: '' if isinstance(x['Resolution_Date'], float) else x['Resolution_Date'], axis=1)\n",
    "target_record_set['Resolution_Name'] = target_record_set.apply(lambda x: '' if x['Resolution_Name'] == 'None' else x['Resolution_Name'], axis=1)\n",
    "target_record_set['Customer Request Type'] = target_record_set['Customer Request Type'].apply(lambda x: '' if pd.isna(x) else x)\n",
    "target_record_set['TIMEORIGINALESTIMATE'] = target_record_set['TIMEORIGINALESTIMATE'].apply(lambda x: '' if pd.isna(x) else x)\n",
    "target_record_set['TIMEESTIMATE'] = target_record_set['TIMEESTIMATE'].apply(lambda x: '' if pd.isna(x) else x)\n",
    "target_record_set['TIMESPENT'] = target_record_set['TIMESPENT'].apply(lambda x: '' if pd.isna(x) else x)\n",
    "\n",
    "\n",
    "\n",
    "print('End formatting records')\n",
    "\n",
    "print(len(target_record_set), ' <-- len prior to updating the watchers process')\n",
    "\n",
    "\n",
    "\n",
    "# need to prep the watcher, worklog and comments for building out the CSV\n",
    "def remove_formatting(text):\n",
    "    # Define a regular expression pattern to match newline and carriage return characters\n",
    "    formatting_pattern = r'[\\n\\r]'\n",
    "\n",
    "    # Use the re.sub() function to replace matching patterns with an empty string\n",
    "    cleaned_text = re.sub(formatting_pattern, ' ', text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# preprocess the comment text to import into jira so there are no ascii or other formatting issues\n",
    "def clean_worklog_text(text):\n",
    "    cleaned = remove_formatting(remove_non_ascii(text)).strip()\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# preprocess the comment text to import into jira so there are no ascii or other formatting issues\n",
    "def clean_comment_text(text):\n",
    "    cleaned = update_email_format(remove_formatting(remove_non_ascii(text))).strip()\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# filter down to the target set to make processing more efficent\n",
    "watchers_target_set = watchers[watchers.issue_id.isin(target_issues_list)].reset_index(drop=True)\n",
    "comments_target_set = commentsdf[commentsdf.Issue_ID.isin(target_issues_list)].reset_index(drop=True)\n",
    "worklog_target_set = worklogsdf[worklogsdf.issueid.isin(target_issues_list)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# preprocess the required worklog fields --> meeting and prep;16/Dec/21 9:36 PM;alolex@vcu.edu;4800\n",
    "worklog_target_set['worklogbody'] = worklog_target_set.apply(lambda x: clean_worklog_text(x.worklogbody), axis=1)\n",
    "#worklogsdf['comment_updated'] = worklogsdf.apply(lambda x: format_date(x.comment_updated), axis=1)\n",
    "\n",
    "# function to preprocess the time format for values less than 60 seconds because Jira requires it in seconds but the min value \n",
    "# that can be set in a project is minutes \n",
    "worklog_target_set['timeworked'] = worklog_target_set['timeworked'].astype(int)\n",
    "def minutes_to_seconds(value):\n",
    "    if value < 60:\n",
    "        return int(60)\n",
    "    else:\n",
    "        return int(value)\n",
    "worklog_target_set['timeworked'] = worklog_target_set.apply(lambda x: minutes_to_seconds(x.timeworked), axis=1)\n",
    "worklog_target_set['timeworked'] = worklog_target_set['timeworked'].astype(str)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preprocess the required commments fields\n",
    "comments_target_set['comment_text'] = comments_target_set.apply(lambda x: clean_comment_text(x.comment_text), axis=1)\n",
    "comments_target_set['comment_updated'] = comments_target_set.apply(lambda x: format_date(x.comment_updated), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(watchers_target_set), '  <-- len of the watcher target set')\n",
    "print(len(comments_target_set), '  <-- len of the comments target set')\n",
    "print(len(worklog_target_set), ' <-- len of the worklog target set')\n",
    "\n",
    "\n",
    "# set a counter to determine the max amount of comments per issueid so this can be used to determine the max number of columns for the csv file\n",
    "max_watchers = 0\n",
    "max_comments = 0\n",
    "max_worklog = 0\n",
    "\n",
    "for l in target_issues_list:\n",
    "    # filter the df based on the issueid\n",
    "    target_watcher_records = watchers_target_set[watchers_target_set.issue_id == l]\n",
    "    target_comment_records = comments_target_set[comments_target_set.Issue_ID == l]\n",
    "    target_worklog_records = worklog_target_set[worklog_target_set.issueid == l]\n",
    "\n",
    "    if len(target_watcher_records) > max_watchers:\n",
    "        max_watchers = len(target_watcher_records)\n",
    "\n",
    "    if len(target_comment_records) > max_comments:\n",
    "        max_comments = len(target_comment_records)\n",
    "\n",
    "    if len(target_worklog_records) > max_worklog:\n",
    "        max_worklog = len(target_worklog_records)\n",
    "\n",
    "print('')\n",
    "print('***********************')\n",
    "print(max_watchers, '  <-- max watcher headers required')\n",
    "print(max_comments, '  <-- max comments headers')\n",
    "print(max_worklog, ' <-- max worklog headers required')\n",
    "\n",
    "\n",
    "\n",
    "# get the column titles in a list so they can be parsed back into the csv\n",
    "column_titles_list = list(target_record_set)\n",
    "\n",
    "\n",
    "watchers_fields = []\n",
    "watcher_count = 0\n",
    "for c in range(max_watchers):\n",
    "    watchers_fields.append('Watcher')\n",
    "    watcher_countt = watcher_count + 1\n",
    "\n",
    "comment_fields = []\n",
    "comment_count = 0\n",
    "for c in range(max_comments):\n",
    "    comment_fields.append('Comment')\n",
    "    comment_count = comment_count + 1\n",
    "\n",
    "\n",
    "worklog_fields = []\n",
    "worklog_count = 0\n",
    "for c in range(max_worklog):\n",
    "    worklog_fields.append('Worklog')\n",
    "    worklog_count = worklog_count + 1\n",
    "    \n",
    "\n",
    "print('')\n",
    "print('***********************')\n",
    "print(len(watchers_fields), ' <-- number of watcher headers created')\n",
    "print(len(comment_fields), ' <-- number of comment headers created')\n",
    "print(len(worklog_fields), ' <-- number of worklog headers created')\n",
    "\n",
    "\n",
    "\n",
    "# build the csv data\n",
    "csv_row_data = []\n",
    "for ids in target_issues_list:\n",
    "\n",
    "    # get a list of the record's data from the processed file\n",
    "    target_records_processed = list(target_record_set[target_record_set['Issue_ID'] == ids].iloc[0])\n",
    "\n",
    "    # filter the df to return the target records\n",
    "    target_watcher_records = watchers_target_set[watchers_target_set.issue_id == ids]\n",
    "    target_comment_records = comments_target_set[comments_target_set.Issue_ID == ids]\n",
    "    target_worklog_records = worklog_target_set[worklog_target_set.issueid == ids]\n",
    "\n",
    "\n",
    "   # build the watcher record and add to the record list\n",
    "    watcher_record = []\n",
    "    for row in target_watcher_records.itertuples():\n",
    "        user_name = row.lower_user_name\n",
    "\n",
    "        watcher_formatted = f'{user_name}'\n",
    "\n",
    "        watcher_record.append(watcher_formatted)\n",
    "\n",
    "    watcher_record_len_processed = make_list_equal_length(watcher_record, len(watchers_fields))\n",
    "\n",
    "\n",
    "   # build the comment record and add to the record list\n",
    "    comment_record = []\n",
    "    for row in target_comment_records.itertuples():\n",
    "        comment = row.comment_text\n",
    "        comment_updated = row.comment_updated\n",
    "        commenter_name = row.commenter_name\n",
    "\n",
    "        comment_formatted = f'{comment_updated};{commenter_name};{comment}'\n",
    "\n",
    "        comment_record.append(comment_formatted)\n",
    "\n",
    "    comment_record_len_processed = make_list_equal_length(comment_record, len(comment_fields))\n",
    "\n",
    "\n",
    "\n",
    "    # build the worklog record and add to the record list\n",
    "    worklog_record = []\n",
    "    for row in target_worklog_records.itertuples():\n",
    "        worklogbody = row.worklogbody\n",
    "        STARTDATE = row.STARTDATE\n",
    "        user_account = row.user_account\n",
    "        timeworked = row.timeworked\n",
    "\n",
    "        worklog_formatted = f'{worklogbody};{str(STARTDATE)};{user_account};{str(timeworked)}'\n",
    "\n",
    "        worklog_record.append(worklog_formatted)\n",
    "\n",
    "    worklog_record_len_processed = make_list_equal_length(worklog_record, len(worklog_fields))\n",
    "\n",
    "\n",
    "\n",
    "    output_row = target_records_processed + watcher_record_len_processed + comment_record_len_processed + worklog_record_len_processed\n",
    "\n",
    "\n",
    "    csv_row_data.append(output_row)\n",
    "\n",
    "\n",
    "output_headers = column_titles_list + watchers_fields + comment_fields + worklog_fields\n",
    "\n",
    "output_path = jconfig['output_path'] # <-------------------------------------------- location of the csv that is loaded into the cloud\n",
    "\n",
    "with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "\n",
    "    write = csv.writer(f)\n",
    "\n",
    "    write.writerow(output_headers)\n",
    "    write.writerows(csv_row_data)\n",
    "\n",
    "print('CSV build complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the CSV records exist in the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the CSV records exist in the cloud\n",
    "output_path = jconfig['output_path'] # <-------------------------------------------- location of the csv that is loaded into the cloud\n",
    "df = pd.read_csv(output_path, header=None, encoding = \"ISO-8859-1\")\n",
    "df.columns = df.iloc[0]\n",
    "df = df.reindex(df.index.drop(0)).reset_index(drop=True)\n",
    "csv_issue_keys = set(list(df['Issue_key']))\n",
    "print(len(csv_issue_keys), ' <-- len of csv issue records')\n",
    "\n",
    "get_all_cloud_issues = jira_session.get_all_issues()\n",
    "cloud_issue_keys = set(list(get_all_cloud_issues.key))\n",
    "print(len(get_all_cloud_issues), ' <-- len of the cloud issue records') #key\n",
    "missing_cloud_records = csv_issue_keys - cloud_issue_keys\n",
    "print(len(missing_cloud_records), ' <-- number of missing csv records from the cloud')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Comments to the OnPrem State\n",
    "### Use the API and the local DB to update the comments' internal/external state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NEED TO UPDATE FROM HERE DOWN\n",
    "\n",
    "df_issue_key_list = list(target_record_set['Issue_key'])\n",
    "print(len(df_issue_key_list), ' <- len of the csv records that were uploaded to the cloud')\n",
    "view_filtered_on_keys = commentsdf[commentsdf.Ticket_Key.isin(df_issue_key_list)]\n",
    "print(len(view_filtered_on_keys), ' <- len of comments records within the records loaded to the cloud')\n",
    "view_filtered_on_keys_list = list(set(list(view_filtered_on_keys['Ticket_Key'])))\n",
    "\n",
    "results_output = []        \n",
    "for k in view_filtered_on_keys_list:\n",
    "    issue_key = k\n",
    "    try:\n",
    "        comment_response = jira_session.get_comments(issue_key)\n",
    "        if len(comment_response) > 0:\n",
    "            results_output.append(comment_response)\n",
    "\n",
    "        else:\n",
    "            print('************')\n",
    "            print(f'Review this error for processing {k} because it should have comments and is not processing as expected.')\n",
    "            print('')\n",
    "\n",
    "    except:\n",
    "        print(f'processing error for {k}, you need to see why this is happening.')\n",
    "\n",
    "\n",
    "concat_results_output = pd.concat(results_output)\n",
    "print(len(concat_results_output), ' <- sanity check to make sure the same amount of comments loaded to the cloud match the comments pulled from the cloud')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def comment_type(text):\n",
    "    try:\n",
    "        if text == '{\"internal\":true}':\n",
    "            output ='Internal'\n",
    "\n",
    "        elif text == '{\"internal\":\"true\"}':\n",
    "            output ='Internal'\n",
    "\n",
    "        elif text == '{\"internal\":false}':\n",
    "            output ='External'\n",
    "        \n",
    "        elif text == '{\"internal\":\"false\"}':\n",
    "            output ='External'\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# remove the records that have None as the value in entity_property_json\n",
    "view_copy = commentsdf.copy()\n",
    "view_copy = view_copy[~view_copy.entity_property_json.isna()]\n",
    "\n",
    "# format the field for downstream processing \n",
    "view_copy['Comment_Type'] = view_copy.apply(lambda x: comment_type(x.entity_property_json), axis=1)\n",
    "\n",
    "# slice out the important stuff to merge \n",
    "view_slice = view_copy[['comment_text','Ticket_Key', 'Comment_Type', 'unique_comment_id']]\n",
    "\n",
    "\n",
    "\n",
    "# def parse_comment_json(value):\n",
    "#     #print(value['content'][0]['content'][0]['text'])\n",
    "#     try:\n",
    "#         output = value['content'][0]['content'][0]['text']\n",
    "#     except:\n",
    "#         output = 'error parsing content'\n",
    "\n",
    "#     return output\n",
    "\n",
    "\n",
    "def parse_comment_json(value):\n",
    "    #print(value['content'][0]['content'][0]['text'])\n",
    "    try:\n",
    "        if len(value['content']) == 1:\n",
    "            output = value['content'][0]['content'][0]['text']\n",
    "\n",
    "        elif len(value['content']) > 1:\n",
    "            for x in value['content']:\n",
    "                if x['type'] == 'paragraph':\n",
    "                    y = x['content']\n",
    "                    for z in y:\n",
    "                        if z['type'] == 'text':\n",
    "                            output = z['text']\n",
    "\n",
    "    except:\n",
    "        output = 'error parsing content'\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "concat_results_output['text'] = concat_results_output.apply(lambda x: parse_comment_json(x.body),axis=1)\n",
    "\n",
    "\n",
    "print(len(concat_results_output), ' <- sanity check to make sure no errors with parsing the json comments to get the text from the cloud')\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Fussy logic assignment of type for records that do not have a direct text match\n",
    "Step 1:\n",
    "Use the dataset that contains the records that were not a direct \n",
    "text match, then group those records by the unique \"Issue_Key\" so there is a collective set of\n",
    "unmatched records from that issue. Then take the set of records that were matched and use that same \n",
    "\"Issue_Key\" to determine the set that was matched for that issue. Then take the comment record set that was\n",
    "taken from the server that contains the original comments and filter on that same \"Isssue_Key\". \n",
    "Determine from the original set and the matched set what records from the original set have not been matched. \n",
    "If all of the remaining, unmatched, original records should be set to \"Internal\" only then change all of the \n",
    "remaining comments that have not been processed to internal\n",
    "\n",
    "If all remaining, unmatched records, are set to \"External\" then do nothing\n",
    "\n",
    "If there is a combination of internal and external records that have not been matched \n",
    "then, check if the API text is in the original record text so a fuzzy match can be attempted\n",
    "by first checking the len() of the len(result_df_error.iloc[0]['body']['content'])\n",
    "then if greater than 1, for each item check if 'type': 'paragraph' \n",
    "then if TRUE then use 'content' to return the list and iterate through the list \n",
    "accessing the 'text' to get that text to see if it is in the original text since the \n",
    "original text could include the text + attachment details\n",
    "\n",
    "Default for internal/external combos will be internal as to prevent any \n",
    "improper disclosure issues. Users can update as needed. \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Remove duplicates from view_slice based on the join keys\n",
    "#view_slice = view_slice.drop_duplicates(subset=['Ticket_Key', 'comment_text'])\n",
    "\n",
    "result_df = concat_results_output.merge(view_slice, left_on=['Issue_Key', 'text'], right_on=['Ticket_Key', 'comment_text'], how='left')\n",
    "result_df_matched = result_df[~result_df.unique_comment_id.isna()]\n",
    "result_df_matched['unique_comment_id']= result_df_matched['unique_comment_id'].astype(int)\n",
    "#result_df_error = result_df[result_df.Comment_Type.isna()]\n",
    "result_df_error = result_df[result_df.comment_text.isna()]\n",
    "\n",
    "print(len(result_df), ' <- len of attempted match by merging the cloud api data with the database data based on text and issue# match')\n",
    "print(len(result_df), ' <- len of exact matched only records')\n",
    "print(len(result_df_error), ' <- len of errors only records')\n",
    "\n",
    "\n",
    "\n",
    "non_matched_list = list(set(list(result_df_error.Issue_Key)))\n",
    "\n",
    "error_hold = []\n",
    "\n",
    "for r in non_matched_list:\n",
    "\n",
    "    error_records = result_df_error[result_df_error.Issue_Key == r]\n",
    "\n",
    "\n",
    "    original_records = view_slice[view_slice.Ticket_Key == r]\n",
    "    original_records_list = list(set(list(original_records.unique_comment_id)))\n",
    "\n",
    "    matched_records = result_df_matched[result_df.Issue_Key == r]\n",
    "    matched_records_list = list(set(list(matched_records.unique_comment_id)))\n",
    "\n",
    "    missing_match_list = list(set(original_records) - set(matched_records_list))\n",
    "\n",
    "    left_over_records = original_records[original_records.unique_comment_id.isin(missing_match_list)]\n",
    "    left_over_records_list = list(set(list(left_over_records.unique_comment_id)))\n",
    "\n",
    "    if 'Internal' and not 'External' in left_over_records_list:\n",
    "        error_records['Comment_Type'] = 'Internal'\n",
    "\n",
    "    elif 'External' and not 'Internal' in left_over_records_list:\n",
    "        error_records['Comment_Type'] = 'External'\n",
    "\n",
    "    # elif 'External' and 'Internal' in left_over_records_list:\n",
    "    #     # create error so these can be checked\n",
    "    #     error_records['Comment_Type'] = 'Internal'\n",
    "\n",
    "    # 'External'\n",
    "\n",
    "    error_hold.append(error_records)\n",
    "\n",
    "error_concat = pd.concat(error_hold).reset_index(drop=True)\n",
    "removed_errors = error_concat[~error_concat.Comment_Type.isna()]\n",
    "print(removed_errors.Comment_Type.unique(), ' <- sanity check to see if None ARE NOT within the dataset as it should not be as ~.isna() was used')\n",
    "\n",
    "fixed_error = pd.concat([result_df_matched, removed_errors])\n",
    "print(len(fixed_error), ' <- len of errors that have been fixed')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "check_remaining_errors = error_concat[error_concat.Comment_Type.isna()]\n",
    "print(len(check_remaining_errors), ' <- len of remaining errors')\n",
    "print(check_remaining_errors.Comment_Type.unique(), ' <- sanity check to see if only None ARE within the dataset because .isna() was used')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "internal_set = fixed_error[fixed_error.Comment_Type == 'Internal']\n",
    "print(internal_set.Comment_Type.unique(), ' <- sanity check to see if only \"Internal\" records are being used')\n",
    "\n",
    "#internal_set = internal_set.iloc[0:5]\n",
    "\n",
    "def update_comment_status(issue_key, comment_id, comment_type, body):\n",
    "    if comment_type == None:\n",
    "        output = 'updated'\n",
    "    elif comment_type == 'Internal':\n",
    "        status_type = 'True'\n",
    "        output = jira_session.update_comment_jsd_public(issue_key, comment_id, status_type, body)\n",
    "    elif comment_type == 'External':\n",
    "        output = 'updated'\n",
    "    return output\n",
    "\n",
    "print(len(internal_set))\n",
    "\n",
    "internal_set['update_status'] = internal_set.apply(lambda x: update_comment_status(x.Issue_Key, x.id, x.Comment_Type, x.body), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 'error' in internal_set.update_status.unique():\n",
    "    print('')\n",
    "    print('')\n",
    "    print('******************************************')\n",
    "    print('')\n",
    "    print('ACTION REQUIRED --->>>> errors found updating comment status')\n",
    "    print('')\n",
    "    print('******************************************')\n",
    "    print('')\n",
    "    print('')\n",
    "elif 'error' not in internal_set.update_status.unique():\n",
    "    print('')\n",
    "    print('')\n",
    "    print('******************************************')\n",
    "    print('')\n",
    "    print('NO errors found updating comment status')\n",
    "    print('')\n",
    "    print('******************************************')\n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Attachements Section\n",
    "### Using the API to load attachments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### this part of the process extracts the required data from the user supplied records\n",
    "#### regarding what attachments they would like to migrate and provides an error report\n",
    "#### if there are not issues that allow the attachment to be posted\n",
    "\n",
    "# loop through the folder that contains the migration records sets from users and extract the contents\n",
    "attachments_folder_path = jconfig['attachments_folder_path'] # <-------------------------------------------- location to directory that contains excel files from agents that lists the attachments they would like migrated   \n",
    "attachments_error_path = jconfig['attachments_error_path'] # <-------------------------------------------- location to directory where error records will be stored  \n",
    "attachments_file_list = list_files_in_folder(attachments_folder_path)\n",
    "attachments_errors_list = list_files_in_folder(attachments_error_path)\n",
    "\n",
    "\n",
    "\n",
    "# delete error records if they exist\n",
    "if len(attachments_errors_list) > 0:\n",
    "    print('deleting prior error files')\n",
    "    for i in attachments_errors_list:\n",
    "        delete_file(i['file_path'])\n",
    "else:\n",
    "    print('no prior error files found to delete')\n",
    "\n",
    "\n",
    "# load the csv with all the jira tickets and use the Issue_key as a filter for the Jira records\n",
    "output_path = jconfig['output_path'] # <-------------------------------------------- location of the csv that is loaded into the cloud\n",
    "df = pd.read_csv(output_path, header=None, encoding = \"ISO-8859-1\")\n",
    "df.columns = df.iloc[0]\n",
    "df = df.reindex(df.index.drop(0)).reset_index(drop=True)\n",
    "csv_issue_key = list(df['Issue_key'])\n",
    "\n",
    "\n",
    "# iterate over the provided attachment excel files and produce an error report\n",
    "# if there are no matching jira issues\n",
    "attachments_hold_df = []\n",
    "for i in attachments_file_list:\n",
    "    df = pd.read_excel(i['file_path'])\n",
    "    df = df.drop_duplicates().reset_index(drop=True)\n",
    "    #df['Issue id'] = df['Issue id'].astype(int)\n",
    "\n",
    "    # check if the attachments have an issue already created and create an error report if they \n",
    "    # do not have an issue to post to\n",
    "    #\n",
    "    matching_records = df[df['Issue key'].isin(csv_issue_key)]\n",
    "    errors = df[~df['Issue key'].isin(csv_issue_key)]\n",
    "    if len(errors) > 0:\n",
    "        file_name = i['file_name']\n",
    "        print('')\n",
    "        print('**********')\n",
    "        print(f'{len(errors)} found in {file_name}')\n",
    "        print('')\n",
    "        attachment_error_path = attachments_error_path + 'no_matching_issue_'+i['file_name']\n",
    "        errors.to_excel(attachment_error_path, index=False)\n",
    "\n",
    "    attachments_hold_df.append(matching_records)\n",
    "    \n",
    "attachments_concat = pd.concat(attachments_hold_df)\n",
    "\n",
    "print('')\n",
    "print('****************')\n",
    "print(len(attachments_concat), ' <-- Number of attachments from team members')\n",
    "print('')\n",
    "print('')\n",
    "attachments_concat = attachments_concat.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# parse the excel file to extract the unique ID for each attachment\n",
    "attachments_concat['attachment_id'] = attachments_concat.apply(lambda x: get_attachment_id(x.Attachment), axis=1)\n",
    "\n",
    "#### end of processing excel attachment records provided by users\n",
    "\n",
    "\n",
    "\n",
    "#### start using the excel attachments records to parse the database records\n",
    "\n",
    "# get a list of the attachment ids so they can be a filter for the database records\n",
    "attachment_concat_attachment_ids = list(set(list(attachments_concat['attachment_id'])))\n",
    "\n",
    "# get all attachment records in the database\n",
    "database_attachment_records = _sys_info_services.get_table_or_view_data('vw_all_jira_attachments_details')\n",
    "database_attachment_records['ID'] = database_attachment_records['ID'].astype(int)\n",
    "database_attachment_records['ID'] = database_attachment_records['ID'].astype(str)\n",
    "# filter only the required records\n",
    "excel_match_records = database_attachment_records[database_attachment_records.ID.isin(attachment_concat_attachment_ids)]\n",
    "\n",
    "project_key_list = ['BRS'] # use the Project_Key for bulk load of project records \n",
    "all_project_attachments = database_attachment_records[database_attachment_records.Project_Key.isin(project_key_list)]\n",
    "\n",
    "# get all attachment records after 1/1/2024\n",
    "database_attachment_records['CREATED'] = pd.to_datetime(database_attachment_records.CREATED)\n",
    "target_date = pd.to_datetime('1/1/2024')\n",
    "current_attachments = database_attachment_records[database_attachment_records.CREATED >= target_date] \n",
    "\n",
    "# get the entire set \n",
    "all_attachment_scope_records = [excel_match_records, all_project_attachments, current_attachments]\n",
    "target_attachment_records = pd.concat(all_attachment_scope_records)\n",
    "print(len(target_attachment_records), ' <-- Number of all project attachment records not de-duped')\n",
    "\n",
    "# drop dups\n",
    "target_attachment_records_no_dups = target_attachment_records.drop_duplicates(subset=['ID']).reset_index(drop=True)\n",
    "print(len(target_attachment_records_no_dups), ' <-- Number of no-dups all project attachment records')\n",
    "\n",
    "# only include attachments that have Jira tickets in the cloud\n",
    "target_attachment_records_no_dups = target_attachment_records_no_dups[target_attachment_records_no_dups.Ticket_Key.isin(csv_issue_key)].reset_index(drop=True)\n",
    "print(len(target_attachment_records_no_dups), ' <-- Number of no-dups records that are available in the cloud')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# requirement to remove ALL of required team members' .sql attachments from the migration\n",
    "name_list = ['T1','T2']\n",
    "user_list_attachments1 = database_attachment_records[(database_attachment_records.Assignee_User_Display_Name.isin(name_list))]\n",
    "user_list_attachments2 = database_attachment_records[(database_attachment_records.Reporter_User_Display_Name.isin(name_list))]\n",
    "user_list_attachments = pd.concat([user_list_attachments1,user_list_attachments2])\n",
    "target_memetype_df = user_list_attachments[user_list_attachments.MIMETYPE == 'application/x-sql']\n",
    "target_memetype_list = list(target_memetype_df.ID) \n",
    "val_mememtype_df = user_list_attachments.copy()\n",
    "val_mememtype_df['sql_check'] = val_mememtype_df.apply(lambda x: find_sql_records(x.FILENAME), axis=1)\n",
    "val_memetype_check = val_mememtype_df[val_mememtype_df.sql_check == 1]\n",
    "val_memetype_check_list = list(val_memetype_check.ID)\n",
    "\n",
    "unique_values_in_each_list = unique_elements(target_memetype_list, val_memetype_check_list)\n",
    "\n",
    "print(len(user_list_attachments), 'len of all users attachements for team members with attachments that need removed')\n",
    "print(len(target_memetype_df), ' len for all their records when filtered for application/x-sql type files using the MIMETYPE field')\n",
    "print(len(val_memetype_check), ' len for all their records when validating using a funcation that checks for .sql files')\n",
    "print(len(unique_values_in_each_list), ' len validating to find if there are unique record ID in each list (sql/meme) indicating issues with ensuring a match for sql records')\n",
    "\n",
    "\n",
    "\n",
    "# remove all of Nevena and Salam's .sql attachment records from the set \n",
    "if len(unique_values_in_each_list) == 0:\n",
    "    #\n",
    "\n",
    "    print('')\n",
    "    print('****************')\n",
    "    print(len(excel_match_records), ' <-- Number of excel attachment records from team members')\n",
    "    print(len(all_project_attachments), ' <-- Number of all project attachment records from project approving ALL attachments')\n",
    "    print((len(excel_match_records)+len(all_project_attachments)+len(current_attachments)), ' <-- Target number of excel and all project attachment records and records after 1/1/2024')\n",
    "    print(len(target_attachment_records), ' <-- Number of all project attachment records from team members')\n",
    "    print(len(target_attachment_records_no_dups), ' <-- Number of no-dups all project attachment records from team members BEFORE to removing team member sql records')\n",
    "\n",
    "\n",
    "    target_attachment_records_no_dups = target_attachment_records_no_dups[~target_attachment_records_no_dups.ID.isin(target_memetype_list)]\n",
    "\n",
    "    print(len(target_attachment_records_no_dups), ' <-- Number of no-dups all project attachment records from team members AFTER to removing team member sql records')\n",
    "\n",
    "    print('')\n",
    "    print('')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    target_attachment_records_no_dups['load_status'] = target_attachment_records_no_dups.apply(lambda x: process_attachments(x.Ticket_Key, x.ID, x.FILENAME, jira_session), axis=1)\n",
    "\n",
    "    error_loading = target_attachment_records_no_dups[target_attachment_records_no_dups['load_status'] == 0]\n",
    "\n",
    "    print(len(error_loading), ' <-- then number of error records from the attachment load process.')\n",
    "\n",
    "else:\n",
    "    print('errors found with validating team member sql records returning unique values for compairing the two methods for identifing sql records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Validation Via Google Drive\n",
    "### Loading validation records into Google Drive so Agents can review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a listing of the attachements that are in-scope for migrate and use this list to find the out of scope records\n",
    "attachment_in_scope_list = list(target_attachment_records_no_dups.ID)\n",
    "attachments_not_in_scope = database_attachment_records[~database_attachment_records.ID.isin(attachment_in_scope_list)]\n",
    "\n",
    "output_path = jconfig['output_path'] # <-------------------------------------------- location of the csv that is loaded into the cloud\n",
    "df = pd.read_csv(output_path, header=None, encoding = \"ISO-8859-1\")\n",
    "df.columns = df.iloc[0]\n",
    "df = df.reindex(df.index.drop(0)).reset_index(drop=True)\n",
    "df['Parent_Issue_ID'] = pd.to_numeric(df['Parent_Issue_ID'], errors='coerce').astype('Int64')\n",
    "df = df.sort_values(by='Parent_Issue_ID', na_position='last')\n",
    "csv_issue_keys = list(df['Issue_key'])\n",
    "\n",
    "all_issues = _sys_info_services.get_table_or_view_data('vw_all_jira_issues') # all jira issues\n",
    "all_issues = all_issues[['Issue_ID', \n",
    "                         'Parent_Issue_ID',\n",
    "                         'Project_Key',\n",
    "                         'Issue_Status',\n",
    "                         'Issue_Type',\n",
    "                         'Issue_Type_Is_SubTask',\n",
    "                         'Created_Date',\n",
    "                         'Reporter_UserName',\n",
    "                         'Creator_UserName',\n",
    "                         'Assignee_UserName',\n",
    "                         'Issue_key']]\n",
    "rds_issues = all_issues[all_issues.Project_Key.isin(['RDS'])]\n",
    "rds_issues_in_scope = rds_issues[rds_issues.Issue_key.isin(csv_issue_keys)]\n",
    "rds_issues_not_in_scope = rds_issues[~rds_issues.Issue_key.isin(csv_issue_keys)]\n",
    "\n",
    "\n",
    "SCOPES = ['https://mail.google.com/',\n",
    "          'https://www.googleapis.com/auth/drive',\n",
    "          'https://www.googleapis.com/auth/spreadsheets']\n",
    "\n",
    "\n",
    "def update_google_sheets_reporting_views(rds_to_migrated,rds_no_migrate, attachments_to_migrate, attachments_no_migrate):\n",
    "    '''\n",
    "    scope:\n",
    "    updates google sheets that are used for reporting views \n",
    "    \n",
    "    args:\n",
    "    none\n",
    "    \n",
    "    output:\n",
    "    updates all required gooogle sheets\n",
    "    \n",
    "    '''\n",
    "\n",
    "    key_path = 'jira_migration_cloud_key.json'\n",
    "    # creditals for gc  \n",
    "    gc = gspread.service_account(filename=key_path)\n",
    "\n",
    "    # google sheet reporting views (make sure to add new reports to listing below for processing)\n",
    "    rds_not_migrated = gc.open_by_url('https://docs.google.com/spreadsheets/d/1smYnX3kArKf8zh2Q3ODlhJK145FV-FhWYXBfWW_BZuE/edit#gid=0')\n",
    "    rds_not_migrated_worksheet = rds_not_migrated.sheet1\n",
    "\n",
    "    rds_migrated = gc.open_by_url('https://docs.google.com/spreadsheets/d/1iRCkrEzaJd4L-aX3W5jgkvfL7xSzbWVNAoo2Ks1yUGY/edit#gid=0')\n",
    "    rds_migrated_worksheet = rds_migrated.sheet1\n",
    "    \n",
    "    utt_not_migrated = gc.open_by_url('https://docs.google.com/spreadsheets/d/1Q7SGlTkBD0eMs9HceFzSL5agPLsVQ5eMKUWZwCzpS7A/edit#gid=0')\n",
    "    utt_not_migrated_worksheet = utt_not_migrated.sheet1\n",
    "\n",
    "    utt_migrated = gc.open_by_url('https://docs.google.com/spreadsheets/d/1G48rPWCmUGuB7c2b_Rve1ctBz1dfw_XITgr2LFeg35Q/edit#gid=0')\n",
    "    utt_migrated_worksheet = utt_migrated.sheet1\n",
    "\n",
    "    attachments_not_migrated = gc.open_by_url('https://docs.google.com/spreadsheets/d/1MGUuUJEZCNYrxLJQYMCFaL8B_ZESS9g2NthbHIoNaAA/edit#gid=0')\n",
    "    attachments_not_migrated_worksheet = attachments_not_migrated.sheet1\n",
    "\n",
    "    attachments_migrated = gc.open_by_url('https://docs.google.com/spreadsheets/d/1du1_6iOP3auEfyWzSOi8nQl-Q9wFNm-RGzRwePGo2gs/edit#gid=0')\n",
    "    attachments_migrated_worksheet = attachments_migrated.sheet1\n",
    "    \n",
    "\n",
    "    \n",
    "    worksheets_list = [rds_not_migrated_worksheet,\n",
    "                       rds_migrated_worksheet,\n",
    "                       utt_not_migrated_worksheet,\n",
    "                       utt_migrated_worksheet,\n",
    "                       attachments_not_migrated_worksheet,\n",
    "                       attachments_migrated_worksheet\n",
    "                       ]\n",
    "    \n",
    "    # clear the sheets for loading new data\n",
    "    for sheets in worksheets_list:\n",
    "        sheets.clear()\n",
    "        \n",
    "     \n",
    "    # helper funtion for preprocessing the data into a format for google sheets \n",
    "    def cast_for_gsheets(df):\n",
    "        # casting as string if not serializable\n",
    "        for column, dt in zip(df.columns, df.dtypes):\n",
    "            if dt.type not in [\n",
    "                np.int64,\n",
    "                np.float_,\n",
    "                np.bool_,\n",
    "            ]:\n",
    "                df.loc[:, column] = df[column].astype(str)\n",
    "        return(df) \n",
    "\n",
    "      \n",
    "    rds_to_migrated_df = cast_for_gsheets(rds_to_migrated)\n",
    "    rds_no_migrate_df = cast_for_gsheets(rds_no_migrate)\n",
    "    attachments_to_migrate_df = cast_for_gsheets(attachments_to_migrate)\n",
    "    attachments_no_migrate_df = cast_for_gsheets(attachments_no_migrate)\n",
    "    \n",
    "    \n",
    "    # add the data to google sheets\n",
    "    set_with_dataframe(rds_migrated_worksheet, rds_to_migrated_df)\n",
    "    set_with_dataframe(rds_not_migrated_worksheet, rds_no_migrate_df)\n",
    "    set_with_dataframe(attachments_migrated_worksheet, attachments_to_migrate_df)\n",
    "    set_with_dataframe(attachments_not_migrated_worksheet,attachments_no_migrate_df)\n",
    "    \n",
    "    print('Google Sheets Reporting Views Have Been Updated')\n",
    "\n",
    "\n",
    "\n",
    "update_google_sheets_reporting_views(rds_issues_in_scope,rds_issues_not_in_scope, target_attachment_records_no_dups, attachments_not_in_scope)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
